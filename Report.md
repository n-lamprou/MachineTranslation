# Machine Translation

## RNN Components and Architectures

### Gated Recurrent Units(GRU)

Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. Although LSTM's are very popular and have great performance, in this project GRUs are used as they have been shown to exhibit better performance on smaller datasets. They have fewer parameters than LSTM, as they don't have an output gate.

<img align=center src="images/gru.png" width="400" height="250" />

### Simple RNN

<img align=center src="images/rnn.png" width="400" height="250" />

### Bidirectional RNN

<img align=center src="images/bidirectional.png" width="400" height="250" />

### RNN with Embedding layer

<img align=center src="images/embedding.png" width="400" height="250" />

### Encoder-Decoder Architecture

<img align=center src="images/encdec.png" width="400" height="250" />

## Performance Comparison

## Additional Work
